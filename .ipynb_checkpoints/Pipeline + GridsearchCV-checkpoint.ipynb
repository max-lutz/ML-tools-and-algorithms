{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = 'https://bit.ly/titanic-train-set'\n",
    "test_url = 'https://bit.ly/titanic-test-set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_url)\n",
    "df_test = pd.read_csv(test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis (easy steps)\n",
    "- Missing data\n",
    "- Categorical data\n",
    "- Text/date data\n",
    "- drop columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "Let's find all the columns with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age         177\n",
       "Cabin       687\n",
       "Embarked      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum().loc[df_train.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age       86\n",
       "Fare       1\n",
       "Cabin    327\n",
       "dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isna().sum().loc[df_test.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train set has missing values in Age, Cabin and Embarqued. <br>\n",
    "The test set has missing data in Age fare and Cabin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique value in column  Name  :  891\n",
      "Unique value in column  Sex  :  2\n",
      "Unique value in column  Ticket  :  681\n",
      "Unique value in column  Cabin  :  148\n",
      "Unique value in column  Embarked  :  4\n"
     ]
    }
   ],
   "source": [
    "cat_cols = [col for col in df_train.columns if df_train[col].dtype==\"O\"]\n",
    "for col in cat_cols:\n",
    "    print('Unique value in column ',col, ' : ',len(df_train[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sex and Embarqued are categorical data <br>\n",
    "Name, ticket and cabin are probably text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarize all this by splitting all the columns in different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['Survived']\n",
    "\n",
    "drop_cols = ['Ticket', 'PassengerId']\n",
    "passthrough_cols = ['Pclass', 'SibSp', 'Parch']\n",
    "\n",
    "num_cols_with_missing_data = ['Age', 'Fare']\n",
    "cat_cols_with_missing_data = ['Embarked']\n",
    "cat_cols_without_missing_data = ['Sex']\n",
    "\n",
    "text_cols_with_missing_data = ['Cabin']\n",
    "text_cols_without_missing_data = ['Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify if there is missing columns or duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    " all_cols = [\n",
    "    target,\n",
    "    drop_cols,\n",
    "    passthrough_cols,\n",
    "    num_cols_with_missing_data,\n",
    "    cat_cols_with_missing_data,\n",
    "    cat_cols_without_missing_data,\n",
    "    text_cols_with_missing_data,\n",
    "    text_cols_without_missing_data\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns badly written : set()\n",
      "Missing columns : set()\n"
     ]
    }
   ],
   "source": [
    "all_cols_set = set()\n",
    "for list_ in all_cols:\n",
    "    for col in list_:\n",
    "        if(col in all_cols_set):\n",
    "            print('Warning, column ',col,' is duplicate')\n",
    "        all_cols_set.add(col)\n",
    "\n",
    "original_cols_set = set(df_train.columns)\n",
    "badly_written_cols = all_cols_set - original_cols_set\n",
    "missing_cols = original_cols_set - all_cols_set\n",
    "print('Columns badly written :', badly_written_cols)\n",
    "print('Missing columns :', missing_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis (advanced steps)\n",
    "- Outliers (removing outliers)\n",
    "- Scaling and transform\n",
    "    - Standard scaler\n",
    "    - Robust scaler\n",
    "    - Log-transform\n",
    "- Feature selection / Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns = target)\n",
    "Y = df_train[target].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 11), (891,))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create preprocessing pipeline\n",
    "check KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cat_preprocessing = make_pipeline(\n",
    "    SimpleImputer(strategy='most_frequent'),\n",
    "    OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#custom function\n",
    "def extract_first_letter(serie):\n",
    "    return pd.DataFrame(serie.str[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_cabin = make_pipeline(\n",
    "    FunctionTransformer(extract_first_letter),\n",
    "    SimpleImputer(strategy='constant', fill_value='MISSING_CABIN'),\n",
    "    OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    ")\n",
    "\n",
    "#test preprocess cabin\n",
    "preprocess_cabin.fit_transform(X['Cabin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = make_column_transformer(\n",
    "    ('drop' ,                                 drop_cols),\n",
    "    ('passthrough' ,                          passthrough_cols),\n",
    "    (SimpleImputer(strategy='median') ,       num_cols_with_missing_data),\n",
    "    (missing_cat_preprocessing ,              cat_cols_with_missing_data),\n",
    "    (OneHotEncoder(handle_unknown='ignore') , cat_cols_without_missing_data),\n",
    "    (preprocess_cabin ,                       'Cabin'),\n",
    "    (CountVectorizer() ,                      'Name')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipeline = Pipeline([\n",
    "    ('preprocessing' , preprocessing),\n",
    "    ('knn', KNN())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pipeline = Pipeline([\n",
    "    ('preprocessing' , preprocessing),\n",
    "    ('ridge', RidgeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessing' , preprocessing),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8249158249158249"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_pipeline.fit(X, Y)\n",
    "pred_knn = knn_pipeline.predict(X)\n",
    "(pred_knn == Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9988776655443322"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_pipeline.fit(X, Y)\n",
    "pred_ridge = ridge_pipeline.predict(X)\n",
    "(pred_ridge == Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pipeline.fit(X, Y)\n",
    "pred_rf = rf_pipeline.predict(X)\n",
    "(pred_rf == Y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7238842508317118, 0.01921603325051323)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score = cross_val_score(knn_pipeline, X, Y, cv=folds)\n",
    "cv_score.mean(), cv_score.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8136902893729208, 0.012023241910489538)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score = cross_val_score(ridge_pipeline, X, Y, cv=folds)\n",
    "cv_score.mean(), cv_score.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8237900947837549, 0.007687619488076668)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score = cross_val_score(rf_pipeline, X, Y, cv=folds)\n",
    "cv_score.mean(), cv_score.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch on KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn= {}\n",
    "knn['pipeline'] = knn_pipeline\n",
    "\n",
    "knn['hyperparameter'] = {}\n",
    "knn['hyperparameter']['knn__n_neighbors'] = [1,3,5,7,9,13,17,21,25]\n",
    "knn['hyperparameter']['knn__weights'] = ['uniform', 'distance']\n",
    "\n",
    "knn['gridsearch'] = GridSearchCV(estimator  = knn['pipeline'],\n",
    "                                 param_grid = knn['hyperparameter'],\n",
    "                                 scoring    = 'accuracy',\n",
    "                                 cv         = folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'knn__n_neighbors': 3, 'knn__weights': 'distance'}, 0.7329044002259746)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn['gridsearch'].fit(X, Y)\n",
    "knn['gridsearch'].best_params_ , knn['gridsearch'].best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch on ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge= {}\n",
    "ridge['pipeline'] = ridge_pipeline\n",
    "\n",
    "ridge['hyperparameter'] = {}\n",
    "ridge['hyperparameter']['ridge__alpha'] = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 100]\n",
    "\n",
    "ridge['gridsearch'] = GridSearchCV(estimator = ridge['pipeline'],\n",
    "                                 param_grid  = ridge['hyperparameter'],\n",
    "                                 scoring     = 'accuracy',\n",
    "                                 cv          = folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'ridge__alpha': 5}, 0.8293955181721172)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge['gridsearch'].fit(X, Y)\n",
    "ridge['gridsearch'].best_params_ , ridge['gridsearch'].best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch on RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf= {}\n",
    "rf['pipeline'] = rf_pipeline\n",
    "\n",
    "rf['hyperparameter'] = {}\n",
    "rf['hyperparameter']['rf__n_estimators'] = [150, 200, 250]\n",
    "rf['hyperparameter']['rf__criterion'] = ['gini', 'entropy']\n",
    "rf['hyperparameter']['rf__max_features'] = ['sqrt', 'auto', 'log2']\n",
    "\n",
    "rf['gridsearch'] = GridSearchCV(estimator    = rf['pipeline'],\n",
    "                                 param_grid  = rf['hyperparameter'],\n",
    "                                 scoring     = 'accuracy',\n",
    "                                 cv          = folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'rf__criterion': 'entropy',\n",
       "  'rf__max_features': 'sqrt',\n",
       "  'rf__n_estimators': 200},\n",
       " 0.832772581758835)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf['gridsearch'].fit(X, Y)\n",
    "rf['gridsearch'].best_params_ , rf['gridsearch'].best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline = make_pipeline(\n",
    "    preprocessing,\n",
    "    RandomForestClassifier(criterion='entropy', \n",
    "                           max_features='sqrt', \n",
    "                           n_estimators=200)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipeline.fit(X,Y)\n",
    "final_pipeline.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
